{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ae7f0c",
   "metadata": {},
   "source": [
    "We use this notebook to randomly select indexes, and we do so by using the data in  Data_v7.csv: \n",
    "- 77 rows: 41 PD + 36 Controls, \n",
    "- 130 columns: Patient, Age, Turning Time (avg between exp 1 and exp 3), 42 indicators x 3 experiments, Label \n",
    "\n",
    "Except for Patient, Age and Label we used all variables in Parkinsonism & Related Disorders. Here we use only 126 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417aef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing, svm, tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df96fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(original,n):\n",
    "    \"\"\"This method randomly selects n elements from original (it works for lists and sets)\n",
    "    and returns a list of selected elements (selected) and the original list/set without \n",
    "    those selected elements (new). The method does not alter the original list/set.\n",
    "    \"\"\"\n",
    "    selected = random.sample(original,n)\n",
    "    new = original.copy()\n",
    "    [new.remove(i) for i in selected]\n",
    "    return selected,new\n",
    "\n",
    "def getError(model,train_x, train_y,test_x, test_y):\n",
    "    train_error,test_error = 0,0\n",
    "    if model == \"SVM\":\n",
    "        clf = svm.SVC()\n",
    "    elif model == \"LR\":\n",
    "        clf = LogisticRegression()\n",
    "    elif model == \"NN\":\n",
    "        clf = MLPClassifier()\n",
    "    elif model == \"kNN\":\n",
    "        clf = KNeighborsClassifier()\n",
    "    elif model == \"DT\":\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "    elif model == \"RF\":\n",
    "        clf = RandomForestClassifier()\n",
    "    clf = clf.fit(train_x, train_y)   \n",
    "    train_error = 1 - clf.score(train_x, train_y)\n",
    "    test_error = 1 - clf.score(test_x, test_y)\n",
    "    return train_error,test_error,clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aadca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "table7 = pd.read_csv(\"Data_v7.csv\", header=0, sep=\",\")\n",
    "types = {\"EPI\":1,\"EPG2019S\":1,\"CONTROLES\":0,\"AsG2019S-\":0} \n",
    "# Eliminate Age and Turning time\n",
    "data = np.array(table7.iloc[:,3:-1])\n",
    "labels = np.array(table7.iloc[:,-1])\n",
    "labels = np.array([types[lab] for lab in labels])\n",
    "# scaling data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c5a0c",
   "metadata": {},
   "source": [
    "The following methodology was used to sample data for training and testing: at each time step, two positive examples anf two negative examples were randomly extracted. The first of each was added to the training set and the other to the test set. This way we have an equal number of positive and negative examples in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c391f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = [i for i,elem in enumerate(labels) if elem==1]\n",
    "neg_labels = [i for i,elem in enumerate(labels) if elem==0]\n",
    "n = min(len(pos_labels),len(neg_labels))\n",
    "train_index, test_index = [],[]\n",
    "for n_samples in range(1,n // 2+1):\n",
    "    x_p,pos_labels = sampling(pos_labels,2) \n",
    "    x_n,neg_labels = sampling(neg_labels,2)  \n",
    "    train_index += [x_p[0],x_n[0]]\n",
    "    test_index += [x_p[1],x_n[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f14565",
   "metadata": {},
   "source": [
    "Since there where more positive than negative examples (41 vs 36), we kept sampling two positive examples while posible, adding the first to the training set and the second one to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad7368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(pos_labels)>=2:\n",
    "    x_p,pos_labels = sampling(pos_labels,2) \n",
    "    train_index += x_p[:1]\n",
    "    test_index += x_p[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a56fe87",
   "metadata": {},
   "source": [
    "Only one example was left out (negative one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e083804",
   "metadata": {},
   "source": [
    "The resulting training and test set are as follows (we save them here for further use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f881809",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = [23, 67, 32, 17, 25, 70, 45, 1, 42, 66, 55, 65, 54, 68, 40, 12, 38, 4, 37, 9, 33, 8, 36, 71, 34, 16, 49, \n",
    "               14, 48, 2, 43, 3, 39, 75, 56, 64, 47, 28]\n",
    "test_index = [21, 63, 57, 0, 44, 6, 50, 15, 52, 62, 27, 18, 53, 72, 20, 76, 41, 60, 51, 10, 58, 5, 24, 11, 19, 69, 31, \n",
    "              7, 26, 61, 59, 73, 30, 74, 29, 13, 46, 35]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5d2d4",
   "metadata": {},
   "source": [
    "Now we want to reserve 65\\% for training (55 examples) and 35\\% for testing (22 examples). Again, we pick them randomly, but we want to make sure that we get both \"easy\" examples and \"difficult\" ones, and in order to decide this we check how many classifiers missclassify each example in a Leave One Out setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db536d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"SVM\",\"LR\",\"NN\",\"kNN\",\"DT\",\"RF\"]\n",
    "wrong_class = {\"SVM\":[],\"LR\":[],\"NN\":[],\"kNN\":[],\"DT\":[],\"RF\":[]}\n",
    "user_idx = range(len(labels))\n",
    "for x in user_idx:\n",
    "    trainLOO = list(set(user_idx).difference(set([x])))\n",
    "    testLOO = [x]\n",
    "    train_x,test_x = data[np.array(trainLOO)],data[np.array(testLOO)]\n",
    "    train_y,test_y = labels[np.array(trainLOO)],labels[np.array(testLOO)]\n",
    "    for model in models:\n",
    "        train_error,test_error,pred_labels = getError(model,train_x, train_y,test_x, test_y)\n",
    "        if test_y!=pred_labels:\n",
    "            wrong_class[model].append(x)\n",
    "difficult = {i:[] for i in range(len(models)+1)}\n",
    "with open(\"Difficult.csv\",\"w\") as f:\n",
    "    f.write(\"Index,\"+\",\".join(models)+\"\\n\")\n",
    "    for i in range(len(labels)):\n",
    "        # a 1 means that the model missclassifies that example \n",
    "        aux = [1 if i in wrong_class[model] else 0 for model in models]\n",
    "        difficult[sum(aux)].append(i)\n",
    "        f.write(\"%s,\"%i+\",\".join([str(j) for j in aux])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa8b88",
   "metadata": {},
   "source": [
    "We will select a test set that has 35% of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2e0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "for i in range(7):\n",
    "    test_set += random.sample(difficult[i],round(0.35*len(difficult[i]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb82b45",
   "metadata": {},
   "source": [
    "This is what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0739c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [3, 55, 24, 31, 29, 53, 30, 9, 25, 75, 49, 56, 44, 41, 36, 6, 60, 23, 54, 57, 63, 47, 20, 26, 2, 38, 61]\n",
    "train_set = [0, 1, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 27, 28, 32, 33, 34, 35, 37, 39, \n",
    "             40, 42, 43, 45, 46, 48, 50, 51, 52, 58, 59, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45da36",
   "metadata": {},
   "source": [
    "There are 51.85\\% positive examples in the test set and 54\\% positive examples in the training set.\n",
    "Not bad proportion, we will keep this one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
